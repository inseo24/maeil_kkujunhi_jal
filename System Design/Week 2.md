### The Google File System 

- The Google File System(이하 GFS)
    - 카테고리 & 주제 : 분산 파일 시스템(Distributed file systems)
    - 자주 사용되는 용어 : Design, reliability, performance, measurement
    - Keywords : Fault tolerance(내결함성, 장애 허용), scalability(확장성), data storage, clustered storage
    
    <details>
    <sumarry> 1. Introduction </summary>
        
    - GFS는 빠르게 증가하는 구글의 데이터 처리 수요를 맞추기 위해 디자인 되고 구현되었다. GFS 이전의 분산 파일 시스템에서 목표로 하던 performance(성능), scalability(확장성), reliability(신뢰성), availability(가용성)를 똑같이 목표로 한다.
    - 하지만, 그 설계는 우리의 애플리케이션에서의 작업 부하와 기술적 환경에 대한 중요한 관찰을 바탕으로 주도됐고, 이건 몇몇 초기의 파일 시스템 디자인의 가정과 꽤 다르다.
    - 과거의 선택을 재검토했고 근본적으로 다른 점을 고민했다.
    
    - 첫번째로, component의 실패는 예외(exception)보다 더 일반적이다.
    - 파일 시스템은 수백, 수천개의 storage machine으로 구성되어 있고, 그 대다수가 저렴한 부품으로 구축되어 비슷한 수의 클라이언트에서 접근하게 된다.
    - 그러다 보니 component 중 사실상 몇몇 개가 작동하지 않거나 발생한 오류에서 복구되지 않을 수 밖에 없다.
    - 애플리케이션에서 발생하는 버그나 OS의 버그, human error, 디스크, 메모리, 커넥터 등에서 발생하는 문제들은 계속 있다.
    - 따라서 지속적으로 모니터링하고 오류를 감지하고(error detection), 내결함성(Fault tolerance), 자동 복구(automatic recovery)가 반드시 시스템에 필요하다.
    
    - 두번째로, 파일은 방대하다. Multi-GB 파일이 보통 그렇다. 각 파일은 일반적으로 웹 문서 같은 많은 애플리케이션 객체를 포함한다.
    - 수십억 개의 빠르게 증가하는 객체를 포함하는 TBs의 데이터 세트로 작업을 한다면, 파일 시스템이 지원 가능함에도 그것보다 작은 KB 크기의 수십억 개 파일도 관리하기 어렵다.
    - 결국 I/O 작업이나 block 크기 같은 디자인 가정이나 매개변수를 재검토해야 한다.
    
    - 세번째로, 대부분의 파일은 덮어쓰기 되는 것보다 새로운 데이터가 추가되어 변하게 된다. 파일 안에서 랜덤 쓰기는 실제로 존재하지 않는다.
    - 한 번 기록이 된 이후로, 읽기만 종종 순차적으로 가능하다. 다양한 데이터가 이런 특성을 갖는다.
    - 몇몇은 데이터 분석 프로그램이 스캔하는 거대한 repository를 구성할 수 있다. 또 다른 일부는 실행중인 애플리케이션에 의해 지속적으로 생성되는 data stream일 수도 있다. 다른 데이터는 그냥 보관용 데이터일 수도 있다. 어떤 건 한 machine에서 생성되고 다른 machine에서 처리되는 중간 결과일 수도 있다.
    - 대용량 파일에 대한 이런 접근 패턴을 고려할 때 데이터 추가는 성능 최적화와 원자성이 보장되지만 클라이언트의 data block를 캐싱하는 장점을 잃어버린다.
    
    - 네번째로, 애플리케이션과 파일 시스템 API가 같이 설계될 경우 유연성이 향상되어 전체 시스템에 도움이 된다.
    - 예를 들어, GFS의 일관성(consistency) 모델을 완화해 애플리케이션에 부담을 주지 않고 파일 시스템을 단순화했다. 또한 여러 클라이언트가 추가적인 동기화(synchronization) 없이 파일을 동시에 추가할 수 있도록 원자적(atomic)으로 데이터를 추가하는 작업을 도입했다. (이에 대해선 백서에 뒷부분에서 더 설명함)
    - 현재 여러 GFS 클러스터가 서로 다른 목적으로 배포되고 있다. 가장 큰 것은 1000개 이상의 스토리지 노드와 300TB 이상의 디스크 스토리지가 있으며 별개의 시스템에서 수백명의 클라이언트가 지속적으로 많이 접근한다.
    
    </details>

    <details>
    <summary> 2. Design Overview </summary>

    <details>
    <summary> 2.1 가정(Assumptions) </summary>

    - 자체적으로 지속적인 모니터링을 해서 정기적으로 component의 오류를 즉시 찾고, 견디고, 복구해야 한다.
    - 시스템은 적당한 수의 대용량 파일을 저장한다. 일반적으로 각 100MB 이상인 수백만개의 파일을 예상하며 다중 GB 파일은 보통의 경우로 효율적으로 관리해야 한다. 작은 파일은 지원되야 하나 최적화 할 필요는 없다.
    - workload는 주로 큰 스트리밍 데이터 읽기와 데이터가 작은 랜덤 읽기로 구성된다. 큰 스트리밍 데이터 읽기에서 개별 작업은 일반적으로 수백 KB 보단 일반적으로 1MB 이상을 읽는다. 동일한 클라이언트의 연속 작업은 보통 파일의 연속적인 영역을 읽는다. 작은 랜덤 읽기는 일반적으로 임의의 offset에서 몇 KB 정도를 읽는다. 성능에 민감한 애플리케이션은 앞뒤로 이동하지 않고 파일을 통해 꾸준히 진행하기 위해 작은 읽기를 일괄적으로 처리하고 정렬하는 경우가 많다.
    - workload에는 파일에 데이터를 추가하는 큰 순차적 쓰기 작업도 있다. 일반적인 크기는 읽기 작업 크기와 비슷하다. 일단 작성된 파일은 거의 다시 수정되지 않는다. 파일의 랜덤 위치에서 작은 쓰기는 지원되지만 효율적인 필요는 없다.
    - 시스템은 동일한 파일에 동시에 추가되는 여러 클라이언트에 대해 잘 정의된 의미 체계(semantics)를 효율적으로 구현해야 한다. 파일은 보통 producer-consumer 큐나 여러 방향의 병합에 사용된다. 컴퓨터당 하나씩 실행되는 수백 개의 producers는 동시적으로(concurrently) 파일을 추가한다. 최소한의 동기화(synchronization) 오버헤드를 주는 원자성이 필수적이다. 파일을 나중에 읽을 수도 consumer가 동시에 읽을 수도 있다.
    - 높고 지속적인 대역폭이 latency가 낮은 것보다 중요하다. 대부분의 애플리케이션은 빠른 속도로 많은 데이터를 처리하는데 프리미엄을 두지만 개별적인 읽기/쓰기에 대해 엄격한 response time을 요구사항으로 하진 않는다.

    </details>

    
    <details>
    <summary> 2.2 Interface </summary>

    - GFS는 친숙한 파일 시스템 인터페이스를 제공하지만, POSIX 같은 표준 API를 구현하지 않는다. 파일은 디렉토리에서 계층적으로 구성되어 있고 경로 이름(path-name)으로 식별된다. 파일을 만들고, 삭제하고, 열고, 닫고, 읽고, 쓰는 일반적인 작업을 지원한다.
    - 또한 GPS에는 snapshot 및 record append 작업이 있다.스냅샷은 낮은 비용으로 파일 또는 디렉토리의 복사본을 생성한다. record append를 사용하면 여러 클라이언트가 개별 클라이언트가 추가하는 것의 원자성을 보장하면서 같은 파일에 동시에 데이터를 추가하는 것도 보장한다.
    - 추가적인 locking 없이, 여러 클라이언트가 동시에 추가할 수 있는 다중 병합(merge) result나 producer-consumer queues를 구현하는데 유용하다.
    - 스냅샷과 레코드 추가는 각각 섹션 3.4, 3.3에서 자세히 설명한다.
    </details>
    
    <details>
    <summary> 2.3 Architecture </summary>

     ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/719c360b-1bb1-4477-9a56-d9a02c2d84ed/Untitled.png)
        
    - GFS 클러스터는 단일 마스터와 여러 chunk 서버로 구성되며 그림 1과 같이 여러 클라이언트에서 액세스 한다.
    - 각각은 일반적으로 user-level의 서버 프로세스를 실행하는 리눅스 시스템이다.
    - 컴퓨터 리소스가 충분하고 불안정한 애플리케이션 코드 실행에도 낮은 신뢰성이 허용된다면 같은 컴퓨터에서 chunk 서버와 클라이언트 모두를 실행하는 건 쉽다.
    - 파일은 고정 크기의 chunks로 나뉜다. 각 chunk는 immutable하고 전역적으로 유니크한 64 비트 chunk handle에 의해 식별되며 그 식별자(chunk handle)는 chunk가 생성되는 시점에 마스터에 의해 할당된다.
    - chunk server는 로컬 디스크에 리눅스 파일로 chunk를 저장하고 chunk handle이나 byte 범위로 지정된 chunk data를 읽거나 쓴다. 안정성(reliability)을 위해, 각 chunk는 다양한 chunk server로 복제된다.
    - 기본적으로 3개의 복제본을 저장하지만, 사용자는 파일 네임스페이스의 다른 영역에 대해 다른 복제 수준을 지정할 수 있다.
    - 마스터는 모든 파일 시스템의 메타데이터를 유지한다. 메타데이터는 네임스페이스, access control 정보, 파일에서 chunk로의 mapping 및 chunk의 현재 위치 정보를 포함한다. 또한 chunk lease 관리, 고아 chunk의 가비지 컬렉션, chunk 서버 간의 chunk migration과 같은 시스템의 전체 활동을 제어한다. 마서터는 heartbeat 메세지로 각 chunk 서버와 주기적으로 통신하며 지시를 내리고 상태를 수집한다.
    - GFS 클라이언트 코드는 각 애플리케이션에 연결되어 있고, 파일 시스템 API를 구현하며 마스터 및 chunk 서버와 통신해 애플리케이션을 대신해 데이터를 읽거나 쓴다. 클라이언트는 메타데이터 작업을 위해 마스터와 상호작용하지만, 모든 데이터가 포함된 통신은 chunk 서버로 직접 이동된다. POSIX API를 제공하지 않으므로 Linux vnode 계층에 연결할 필요가 없다.
    - 클라이언트와 chunk 서버 모두 파일 데이터를 캐싱하지 않는다. 대부분의 애플리케이션이 대용량의 파일을 스트리밍하거나 캐시하기에 너무 큰 작업 세트를 갖고 있어서 클라이언트 캐시는 캐시할만한 장점이 없다. 캐시를 하지 않으면 캐시 일관성 문제가 없어져서 전체 시스템이 단순해진다.
    - 클라이언트는 메타데이터를 캐시하지만, chunk 서버는 파일 데이터를 캐시할 필요가 없다. chunk는 로컬 파일로 저장되기 때문에 Linux의 buffer cache는 이미 자주 액세스 되는 데이터를 메모리에 보관하고 있다.
    </details>
    
    <details>
    <summary> 2.4 Single Master </summary>

    - Single Master를 사용하면 디자인이 크게 단순해지고 Master는 전역 데이터를 이용해 정교하게 chunk를 배치하거나 복제(replication) 결정을 할 수 있다. 하지만 bottleneck이 되지 않게 읽기과 쓰기에 관여하는 것을 최소화해야 한다.
    - 클라이언트는 절대로 마스터를 통해 파일 데이터를 읽고 쓰지 않는다. 대신 마스터에서 어떤 chunk server에 접속해야 하는지 물어본다. 제한된 시간 동안 이 정보를 캐싱하고 다른 작업을 위해 chunk server와 직접 상호 작용한다.
    - 그림 1을 참고해 간단한 읽기에 대한 상호 작용을 설명해보자.
        - 먼커 클라이언트가 고정 크기의 chunk를 사용해 애플리케이션에서 지정한 파일 이름과 byte offset을 파일 내의 chunk index로 바꾼다.
        - 그 다음 마스터에게 파일 이름과 chunk index를 포함해 요청을 보낸다.
        - 마스터는 해당 chunk handler과 replication 위치로 응답한다.
        - 클라이언트는 파일 이름과 chunk index를 키로 사용해 이 정보를 캐싱한다.
        - 그런 다음 클라이언트는 가장 가까운 복제본 중 하나에 요청을 보낸다.
        - 요청은 chunk handle과 해당 청크 내의 바이트 범위를 지정한다.
    - 캐시된 정보가 만료되거나 파일이 다시 열릴 때까지 클라이언트와 마스터 사이에 상호작용은 필요하지 않다.
    - 실제로 클라이언트는 일반적으로 동일한 요청에서 여러 chunk를 요청하고 마스터는 요청된 chunk 바로 다음에 chunk에 대한 정보를 포함할 수 있다.
    - 이 추가 정보로 인해 실질적으로 추가 비용 없이 여러 클라이언트-마스터 상호작용을 피할 수 있게 된다.

    </details>
    
    
    <details>
    <summary> 2.5 Chunk Size </summary>

    - Chunk Size는 중요한 디자인 매개변수 중 하나다. 우리는 보통 파일 시스템의 block size보다 큰 64 MB를 선택했다. 각 chunk 복제본(replica)은 chunk server나 필요시 확장된 서버에 평범한 리눅스 파일처럼 저장된다. Lazy space allocation은 내부 단편화로 인한 공간 낭비를 줄이며 아마도 이런 큰 chunk size에 대해 가장 큰 반대 이유가 된다.
    - chunk size가 크면 몇 가지 장점이 있다.
        - 첫번째로, 클라이언트가 마스터와 상호작용할 필요성을 줄인다. 읽기와 쓰기 시 모두 같은 청크 위치 정보를 한 번의 최초 요청만 필요하기 때문이다. 이 감소가 매우 중요한데 대부분의 애플리케이션의 읽기 쓰기가 순차적이기 때문이다. 작은 사이즈의 랜덤 읽기라 하더라도, 클라이언트는 편하게 모든 chunk 위치 정보를 캐싱할 수 있다.
        - 두번째로 chunk size가 크기 때문에 많은 작업이 주어진 chunk로 처리될 수 있어, 네트워크 오버헤드가 줄어들게 된다.(TCP 연결로 인한)
        - 세번째로 마스터에 저장된 메타데이터의 크기를 줄일 수 있다. 이를 통해 메타데이터를 메모리에 보관할 수 있으며 이는 2.6.1에서 나옴
    - 반대로, chunk size가 크기 때문에 lazy space allocation을 하더라도 단점이 있다.
        - 작은 파일은 적은 수의 chunk로 구성되며 아마도 그냥 1개일 것이다. 이런 chunk를 저장하는 chunk server는 많은 클라이언트가 동일한 파일에 접근할 경우 hotspot이 될 수 있다. 실제로 hotspot은 애플리케이션이 대부분 큰 다중 chunk를 순차적으로 읽기 때문에 주로 문제가 되지 않았다.
        - 하지만 GFS가 batch-queue 시스템에서 처음 사용됐을 때, hotspot이 발생했다. 실행 파일이 GFS에 단일 chunk 파일로 기록된 다음 수백만 대의 시스템에서 동시에 시작됐다. 이 실행 파일을 저장하는 몇 안되는 chunk server는 수백 건의 동시 요청으로 과부하가 걸렸다.
        - 이런 실행 파일을 더 높은 replication factor로 저장하고 batch-queue 시스템이 애플리케이션 시작 시간에 시간차를 두도록 해서 문제를 해결했다.
    - 잠재적으로 장기적인 해결책은 클라이언트가 이런 상황에서 다른 클라이언트의 데이터를 읽을 수 있게 하는 것이다.

    </details>
    
    <details>
    <summary> 2.6 Metadata</summary>
    
    - 마스터는 파일 및 chunk namespace, 파일에서 chunk로의  mapping, 각 chunk의 replication 위치 등 3가지 주요 메타데이터를 저장한다.
    - 모든 metadata는 마스터의 메모리에 보관된다. namespace와 file-chunk mapping 데이터는 마스터의 로컬 디스크에 저장되고 원격 시스템에 복제되는 작업 로그에 변화(mutation)를 기록해 영구적으로 유지된다.
    - 로그를 사용하면 마스터 충돌 시 불일치 위험 없이 간단하고 안정적으로 마스터 상태를 업데이트할 수 있다.
    - 마스터는 chunk location 정보를 지속적으로 저장하지 않고, 마스터 시작 시와 chunk server가 클러스터에 합류할 때마다 각 chunk server에게 chunk 정보를 물어본다.
    
    - 2.6.1 In-Memory Data Structures
        - 메타데이터가 메모리에 저장되기 때문에, 마스터 작업은 빠르다. 또한 백그라운드에서 전체 상태를 주기적으로 마스터를 스캔하기도 쉽고 효율적이다.
        - 주기적인 스캐닝은 chunk 가비지 컬렉션, chunk 서버 장애 시 재복제(re-replication), chunk 서버 전체에서 부하와 디스크 공간 사용의 균형을 맞추기 위한 chunk migration을 구현하는데 사용된다. 이에 대해서 4.3, 4.4에서 자세히 설명함
        - 이 메모리 전용 접근 방식에 대한 1가지 잠재적인 문제는 chunk 수와 전체 시스템 용량이 마스터의 메모리 양에 따라 제한된다는 점이다. 그리 심각한 제한은 아니다. 마스터는 각 64MB chunk에 대해 64 Byte 미만의 메타데이터를 유지한다. 대부분의 파일에는 많은 chunk가 포함되어 있고 마지막 chunk만 부분적으로 채워질 수 있기 때문에 대부분의 chunk가 가득차 있다. 마찬가지로 file namespace 데이터는 prefix compression을 사용해 파일 이름을 압축해 저장하기 때문에 일반적으로 파일당 64byte 미만이 필요하다.
        - 더 큰 파일 시스템을 지원해야 하는 경우 마스터에 추가 메모리를 추가하는 비용 정도는 메모리에 메타데이터를 저장하면서 얻는 단순성, 안전성, 성능, 유연성에 비하면 작은 비용이다.
    - 2.6.2 Chunk Locations
        - 마스터는 어떤 청크 서버가 주어진 청크에 대해 복제본을 갖고 있는지 지속적인 기록을 갖고 있진 않는다. 시작 시 해당 정보에 대한 청크 서버를 polling 한다. 마스터는 모든 청크 배치를 제어하고 정기적으로 하트비트 메시지를 이용해 청크 서버 상태를 모니터링해 이후에도 최신 상태를 유지할 수 있다.
        - 계속 마스터에 정보를 유지하는 것보다 시작 시 청크 서버에 데이터를 요청 & 주기적으로 요청하는 게 훨씬 간단하다.
        - 이렇게 하면 청크 서버가 클러스터에 포함되든 삭제되든, 이름이 변경되든 실패하든, 재시작하든 마스터 및 청크 서버를 동기화 상태로 유지할 수 있다. 수백 대의 서버가 있는 클러스터에서 이런 이벤트들은 너무 자주 발생한다.
        - 이 디자인 결정을 이해하는 다른 방법으로 청크 서버가 자신의 디스크에 있는 청크에 대한 최종 결정권을 갖고 있음을 알아야 한다. 청크 서버의 오류로 인해 청크가 스스로 없어지거나 운영자가 청크 서버 이름을 바꿀 수 있기 때문에 마스터에서 이 정보를 일관성 있게 유지하려고 시도하는 게 의미가 없다.
    - 2.6.3 Operation Log(작업 로그)
        - 작업 로그에는 중요한 메타데이터의 변경 내역이 기록된다. GFS의 핵심이다. 메타데이터의 유일한 역구 기록일 뿐만 아니라 동시에 진행되는 작업의 순서를 정의하는 논리적 타임 라인의 역할도 한다. 파일 및 청크와 그 버전은 모두 생성된 논리적 시간으로 고유하고 영구적으로 식별된다.
        - 작업 로그는 중요하기 때문에 안정적으로 저장하고 메타데이터 변경 사항이 영구적으로 유지될 때까지 클라이언트에 변경 사항이 표기되지 않도록 해야 한다. 그렇지 않으면 청크 자체가 남더라도 전체 파일 시스템이나 최근 클라이언트 작업을 잃게 된다. 따라서 여러 원격 시스템에 복제하고 해당 로그 레코드를 로컬 및 원격으로 디스크에 flush한 후에만 클라이언트 작업에 응답한다. 마스터는 플러시하기 전에 여러 로그 레코드를 일괄 처리해 플러시 및 복제가 전체 시스템 처리량에 미치는 영향을 줄인다.
        - 마스터는 작업 로그를 재생해서(replaying) 파일 시스템의 상태를 복구한다. 시작 시간을 최소화하려면 로그를 작게 유지해야 한다. 마스터는 로그가 일정 크기 이상으로 커질 때마다 자신의 상태를 체크포인트로 해서 로컬 디스크에서 최신 체크포인트를 불러와 제한된 수의 로그 레코드만 재생해 복구할 수 있도록 한다. 체크포인트는 메모리에 직접 매핑할 수 있고 추구적인 분석 없이 네임스페이스 조회에 사용할 수 있는 형식이다.(작은 B-tree) 이렇게 해서 복구 속도가 빨라지고 가용성이 향상된다.
        - 체크포인트를 만드는데 시간이 걸릴 수 있기 때문에 마스터의 내부 상태는 새로 들어오는 mutation을 지연하지 않으면서 새로운 체크포인트를 생성할 수 있는 방식으로 구성된다. 마스터는 새 로그 파일로 전환하고 별도의 스레드에 새 체크포인트를 생성한다. 새 체크포인트에는 전환하기 전의 모든 mutation이 포함된다. 수백만 개의 파일이 있는 클러스터의 경우 1분 정도만 만들 수 있다. 완료되면 로컬 및 원격으로 디스크에 기록된다.
        - 복구에는 최신의 전체 체크포인트와 후속 로그 파일만 필요하다. 이전 체크포인트와 로그 파일은 자유롭게 삭제할 수 있지만 큰 문제를 방지하기 위해서 몇 개는 보관해야 한다. 복구 코드가 불완전한 체크포인트를 감지하고 건너뛰기 때문에 체크포인트를 지정하는 도중에 오류가 발생해도 정확성에 영향을 미치지 않는다.

    </details>
        
    
    <details>
    <summary> 2.7 Consistency Model </summary>

    - 2.7.1 Guarantess by GFS
            
            
    - 2.7.2 Implications for Applications
    </details>

    </details>

    <details>
    <summary> 3. System Interactions </summary>

    </details>
